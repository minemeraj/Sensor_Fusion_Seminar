\chapter{Applications}

\section{Applications}
In the current age, Sensor fusion is used in the systems which were only found in science fiction books and were just an imagination (e.g. self driving car). It has become the key component for moving object detection and tracking\cite{Cho_2014, Chavez_Garcia_2016}, obstacle detection\cite{Shinzato_2014} which are the most challenging task in the application domain of autonomous driving cars. In the following parts of this paper, some of the successful applications of sensor fusion are described.


\section{Multiple Sensor Fusion for Moving Object Detection and Tracking}
The intelligent vehicles are now becoming capable of instruct themselves to drive in very dynamic and unstructured environment. But driving in such an environment increases uncertainty. One of the uncertain task is moving object detection and tracking. Perceiving the environment involves the selection of different sensors to obtain a detailed description of the environment and an accurate identification of the objects of interest\cite{Chavez_Garcia_2016}. So, Fusion of multiple sensors could decrease the uncertainly in moving object detection task. In the following, two different approaches are described for moving object detection which are different in the view point of the sensors.

\subsection{Front View Approach}
In \cite{Chavez_Garcia_2016}, an advanced driver assistance systems (ADAS) is introduced whoose main task is to help the drivers to avoid dangerous situations. ADAS assists with warning messages in dangerous driving situations (e.g., possible collisions), activation of safety devices to mitigate imminent collisions, autonomous maneuvers to avoid obstacles, and attention-less driver warnings\cite{Chavez_Garcia_2016}. There are two major tasks to perceive the environment. The task simultaneous localization and mapping (SLAM) is one of and another one is to detect and track the moving objects (DATMO). The model of perception task is shown in figure \ref{fig:datmoslam}.
\begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{src/pic/datmo_slam_arch.png}
  \caption{General Architecture of perception task. \cite{Chavez_Garcia_2016}}
  \label{fig:datmoslam}
\end{figure}
\begin{description}
    \item[SLAM:] Could be defined as follows:\\
    A self driving car is moving in an unknown environment and it starts moving from a known location. Meanwhile, it's motion is uncertain which makes the determination of it's global position more difficult. As it is moving, continuously it perceives the environment. In this situation, SLAM is the procedure of building a map of the environment while simultaneously calculating the cars position relative to the map\cite{Thrun2008}. More formally we can say, the SLAM problem asks if it is possible for a autonomous agent to move to an unknown location in an unknown, dynamic or unstructured environment and for the agent to incrementally build a consistent map of this environment while simultaneously determining its location within this map\cite{Bailey:1638022}.
    \item[DATMO:] While SLAM creates a map of static objects including the position of the agent, DATMO uses the map  of static objects generated by SLAM \cite{Azim:6232303} to determine the object to be moved or static\cite{vu2008mapping}. On the other hand, by performing DATMO it is possible to get the position of moving objects around, track them and even predict the future behaviour of those moving objects\cite{vu2008mapping}.
\end{description}

It is assumed that the SLAM is a solved task. As a result it is more reasonable to focus on the DATMO\cite{Chavez_Garcia_2016}. DATMO is performed with an Evidential fusion approach where object classification is the key component. Additionally, Evidential fusion handle uncertainty of sensor detection. Actual goal is to find a list of moving objects with their velocity which should improve the ADAS.

Handling incomplete environment data is one of the important factors of assistance systems which leads to wrong perception. The reasons for incomplete sensor data are already described in the introduction section of this paper. Including incomplete information, there is another problem exists in the tracking process. The tracking process expects that all inputs are moving objects. But in the real world, environment consists of both moving and static objects. So, moving object detection is a very difficult task of moving object tracking system and various number of sensors need to work together to perform the task.

\subsubsection{Environment Setup}
In the experiment \cite{Chavez_Garcia_2016}, a Lancia Delta car is decorated with necessary equipment such as a processing unit to process the image and create moving object list, some components which is responsible for driver interaction and the front - facing sensors i.e., Lidar, Camera and Radar. So the sensors only cover the front view of the vehicle as shown in figure \ref{fig:sensorcovarage}. An IBEO Lux laser scanner is used as the lidar sensor which can deliver a 2D list of impact points and it can cover all the points in the range of 200m. The radar is used to detect moving targets. It could detect targets within 150m and the velocity range up to 250kph. The camera would capture black and white images.

\subsubsection{System Architecture}
The fusion process is performed in a Perception System (PS) which is developed according to improve the efficiency and increase the quality of sensor data. It tries to concentrate on object detecting and tracking to improve the data quality. The PS aims at detecting, classifying and tracking a set of moving objects of interest that may appear in front of the vehicle\cite{Chavez_Garcia_2016}. The structure of the  PS is shown in Figure \ref{fig:PS}. In this system, the fusion module is taking input directly from the three sensors. Initially lidar, radar and camera get the input from the environment and create a list of detected objects for each sensor. Those three lists are then provided to the fusion module. Each environment object is demonstrated by their position, size and evidence distribution of class hypotheses. On the other hand, classifications are done by the shape, relative speed and visual appearance of the detected objects. Precisely, the lidar and radar is performing the task of detection while camera is extracting the features of the detected objects to classify them. The fusion module fused the three list of objects and combined them into one list  where each object contains combined description. The ultimate output of the fusion method is provided to the tracking module to calculate the moving objects states which is actually the final output of the DATMO task. Three sensors are contribution in the task of moving object detection and tracking. The next sections are going to describe briefly how the the sensors prove information for detection and tracking.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{src/pic/PS.png}
        \caption{}
        \label{fig:PS}
    \end{subfigure}
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{src/pic/sensor_covarage_front.png}
        \caption{}
        \label{fig:sensorcovarage}
    \end{subfigure}
    \caption{\ref{fig:PS}:Sensor Fusion model, \ref{fig:sensorcovarage}:Coverage of the sensors}
    \label{fig:group1}
\end{figure}

\subsubsection{Moving Object Detection}
The process of three individual sensors for Moving object detection are described bellow:
\begin{description}
    \item[Lidar Processing:] Lidar is used as the leading sensor in the setup as it can produce high resolution data with more accurate result. The principal task of lidar is to get exact information of the shape of the moving object in front of the vehicle. Though the main pourpose is to focus on DATMO task, the SLAM component is also implemented to get the map of objects and vehicles position in the map\cite{Chavez_Garcia_2016}. By the lidar data a 2D Bayesian occupancy grid map is created. Each cell in the map is filled with or not by an object where vehicles location if found by Maximum Likelihood approach. Each time the sensor updates, the map is recalculated and the vehicle position is also re-estimated. Besides, lidar based detection also works with the help of the grid map. If the a cell of grid map is occupied with an obstacle which was previously free cell, then it is said to be a moving object. In the contrast, if a cell is occupied with an object which was also previously occupied, then it is said to be a static object. The SLAM process is completely independent from the moving object detection task.
    \item[Camera Images:] The camera images are processed to extract the visual features and classification.
    \begin{enumerate}
        \item{Visual Representation:} The Histograms of Oriented Gradients (HOG) descriptor is taken as the core of vehicle and pedestrian visual representation as it has already shown promising results\cite{Chavez_Garcia_2016}. The main goal is to extract the graphical details of the particular area of the image which is important to extract for future use to find out the existence of the object of interest. A sparse version of the HOG descriptor (S-HOG) is proposed to use instead of actual HOG descriptor which focuses on specific areas of an image patch \cite{Chavez_Garcia_2016}. S-HOG reduces the common high-dimensional HOG descriptor to explains the selected block of the image of different class.
        \item{Object Classification:} Detecting the region of interest (ROI) from camera image is a performance degrading task. On the other hand, the ROI is already available from lidar sensor and that is why already provided ROI is used in the camera images to specify object region. Every ROI is processed to extract the visual features and a classifier is applied to decide if an object of interest is inside the ROI. As the speed and quality of the result is totally depend on the choice of the classifier, an optimized and boosting-based learning algorithm is implemented which is called discrete Adaboost\cite{Chavez_Garcia_2016}. This algorithm combines many week classifiers and combine them to make the powerful one which increase the performance. For each class of interest e.g., pedestrian, bike, car, truck, a binary classifier was trained off-line to identify object (positive) and non-object (negative) patches\cite{Chavez_Garcia_2016}.
    \end{enumerate}
    \item[Radar Targets:] Radar sensor has a build in method to detect moving objects. Radar creates a list of n moving objects and send the list to perception approach. Each element of the list contains the range, direction of moving and relative speed of the detected target. There are some limitation of Radar sensor. One problem is, Radar also could include static objects in the list. Sometimes the week objects could not always detected by the Radar which leads to miss detection. Because of various problems, the targets are tracked using constant velocity, acceleration and turning models represented by Interactive Multiple Model (IMM).
\end{description}

\subsubsection{Moving Object Classification}
The information about kinetic state of the objects with classification is determined in detection level. The descriptions found from detection level is very useful for better estimation of object's motion and removes number of miss leading detection. In detection level, only one class is selected for the object which means it is not possible to fix the classification if the initial one is wrong. It is necessary to keep more than one approximate classifier so that in case of wrong classification, it is easy to get the correct classifier.

A composite representation is formed by two parts: kinetic + appearance\cite{Chavez_Garcia_2016}. Kinetic description includes position and shape information in a 2D space. Appearance description includes an evidence distribution m($2^{\Omega}$) for all possible class hypothesis where $\Omega$= \{pedestrian, bike, car, truck\} is the frame of discernment representing the classes of interest\cite{Chavez_Garcia_2016}. The fusion module mainly detect and track the objects in the from of $\Omega$ and perform the classification. The process of three individual sensors for Moving object classification are described bellow:
\begin{description}
    \item[Lidar Sensor:] In the detection task, the shape of the moving object is already found. So, the shape of the object can be used for kinetic representation. The modeling of the moving objects are done by a box \{x,y,w,l,c\}, where x and y are the center of the box, w and l are the width and length according to the class of object \cite{Chavez_Garcia_2016}. But if the moving object is very small, the box model doesn't fit. For example pedestrians are not able to modeled by a box rather it is assume to be a 2D object. So for small objects a point model \{x,y,c\} is suited well where x, y are the 2D coordinate of center of the object and c is the class. Classification of objects is done based on the shape and size of the object which follows a fixed fitting-model approach. It is hard to get the exact classification of moving object because the visibility is temporary. For example, if the width of an object in less than a threshold, it is classified as a bike or pedestrian.
    
    In this way, it is necessary to know the threshold for all classes. The thresholds are defined for examining the size of typical passenger cars, trucks and motorbikes sold in Europe\cite{Chavez_Garcia_2016}. In this experiment, instead of keeping only one class decision, a basic belief assignment $m_{1}(A)$ for each $ A \in \Omega $ is defined, which describes an evidence distribution for the class of the moving object detected by lidar\cite{Chavez_Garcia_2016}.
    \item[Camera Sensor:] In the detection phase, lidar provides a set of ROIs which are used to generate hypothesis that the objects are located in the image at the place of the set of ROIs. To verify the hypothesis, off-line classifiers are applied to classify different objects. The camera image classification extracts more information from a ROI by dividing the ROI into several sub regions. After classification of each ROI has done, a belief assignment $m_{c}$ is calculated which represents the evidence distribution for the classes hypotheses\cite{Chavez_Garcia_2016}.
    \item[Radar Sensor:] This sensor is mainly used for moving object detection. But if other sensors can not classify then this sensor tries to classify moving objects. As radar can determine the speed, speed is used for classification. The slowest speed of the vehicle classes is estimated statistically and determine a threshold for each class. That means for cars, trucks, bikes and pedestrians there is a threshold and if the moving object move in a speed of the threshold than the object classified as the corresponding class. Finally, the basic belief assignment is calculated based on the speed classification\cite{Chavez_Garcia_2016}.
\end{description}

\subsubsection{Sensor Fusion}
Once the DATMO and object classification task is finished, the composite object representation are ready for fusion. A multi-sensor fusion framework is proposed at the detection level which not only depend on three sensors but also many sensors can be added to the framework to extend the fusion process\cite{Chavez_Garcia_2016}. The combination of different representation of same object at detection level increase the reliability and reduce incompleteness, uncertainty of the detection result. For fusion approach it is important that for example an object getting from three sensors are same or are associated with each other. If an object $i$ is detected in sensor $a$ and $b$ and the detected objects are presented as $a_{i}$ and $b_{i}$ respectively. After detection the following things can be happen,
\begin{itemize}
    \item $a_{i}$ and $b_{i}$ are the same object; i.e., P($a_{i}$, $b_{i}$) = 1
    \item $a_{i}$ and $b_{i}$ are not the same object; i.e., P($a_{i}$, $b_{i}$) = 0
    \item ignore the association; i.e., P($a_{i}$, $b_{i}$) = $\Omega$
\end{itemize}

The object associations can be calculated by finding the similarity measures between them. Sensor $a$ and $b$ could provide different kind of description about the object. The description could be the position, shape or appearance information such as class. The similarity can be calculated based on the position or appearance information of the detected object. After two objects are decided to be same, the fusion method combines the object representation by fusing the information received from multiple sensors\cite{Chavez_Garcia_2016}. The fused object with kinetic and appearance information, are passed to the tracking system to estimate object motion model. The objects, for which no association is found, are simply removed by the tracking process.

\subsubsection{Experimental Outcome}
The perception System (PS) is tested in both highway and urban area with high traffic scenarios. In both areas, all vehicles including coming ones, are detected and tracked\cite{Chavez_Garcia_2016} as shown in figure \ref{fig:Experimental_PS}. Another good thing is that it could found out the static objects as well. From the figure. \ref{fig:Experimental_PS} it is shown that the vehicle speed are also detected. It is possible only because of fused information. Radar providing the speed and direction of the vehicle where lidar gives shape information and camera gives more detailed image description like hand or leg of a pedestrian. The class of object is improved by fused information from the three different sensors.
\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{src/pic/Experimental_PS.png}
  \caption{Results of PS 1: Highway, 2: Urban Areas \cite{Chavez_Garcia_2016}}
  \label{fig:Experimental_PS}
\end{figure}

\subsection{Multiple View Approach}
In \cite{Cho_2014}, a previously build perception system is improved to adopted the real world driving experience. The previous perception system was designed for a robotic vehicle which could operated on a simple, urban environment setup having no obstacle objects with limited vehicle interactions \cite{Cho_2014}. But in the real world, an autonomous driver agent have to be aware of all possible risky situations such as prevent coalition with nearby pedestrians or vehicles. To have safety driving experience, it is very obvious to detect and track the moving objects. In this paper\cite{Cho_2014}, the previously developed perception system and the moving object tracking system are redesigned to ensure the safety and reliability. Even new sensors are placed in different view of the vehicle.


\subsubsection{Environment Setup}
The sensors are configured in a vehicle in a manner to track all the moving objects, cover the region around the vehicle within a certain range and take advantage of vehicle's build in sensors. According to those criteria the sensors are placed like shown in Figure \ref{fig:Sensor_Setup_Multiple_View} (a). All the sensors are set in the vehicle body in a way that the sensors are not visible from outside. Precisely, six radars, six LIDARs and three cameras are installed in the experiment vehicle. One radar is paired with a LIDARs at different height. Robust number of sensors are used to get maximum reliability with an extended range of sensor coverage. According to \cite{Cho_2014} the current setup, any object which exists in the range of 200 meters of the vehicle can be observed by any of the sensors. Any object in the range of 60 meters could be observed by at least two sensors either radar and LIDAR or radar and camera. The coverage of sensors are visualized in the Figure \ref{fig:Sensor_Setup_Multiple_View}(b). There are three cameras in three different-looking faces. One camera is installed in froward facing approach inside the front window. Another camera is installed in the rear bumper to provide the front and back side of perspective images. The third one is a thermal one which is able to sense even in night or foggy weather. Another advantage of having wide range of sensors is that the spots are small enough that no vehicle will be overlooked\cite{Cho_2014} Overall there are 14 sensors are installed in the vehicle.

\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{src/pic/Sensor_Setup_Multiple_View.png}
  \caption{\cite{Cho_2014}}
  \label{fig:Sensor_Setup_Multiple_View}
\end{figure}

\subsubsection{System Architecture}
\begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{src/pic/Fusion_Arch_Multiple_View.png}
  \caption{\cite{Cho_2014}}
  \label{fig:Fusion_Arch_Multiple_View}
\end{figure}
The new redesigned perception system have two parts i.e., sensor and fusion layer as shown in figure \ref{fig:Fusion_Arch_Multiple_View}. Dividing the system into two independent module makes it possible to develop them independently based on an interface without knowing each others internal functionality. There is a common communication channel available for data exchange between those two modules. Each time new raw sensor data is available, reader extract features such as lines or corners and push the data to the common communication channel. The shared data is accessed any time by the higher level module i.e., fusion module. In fusion layer, the sensor measurements are processed as unit of measures where each object measured as a box model, but they are presented differently\cite{Cho_2014}. For example, a radar provides 2D position and velocity of an object. Radar data is represented in an array where time as the index of the array. Radar measurement in $k^{th}$ second  is:
\begin{equation}
z^{R}(k) = \{r_{1},r_{2},...,r_{p}\} \ here, r_{i} = [\mathbf {x \ y \ \dot{x} \ \dot{y}}]^{T} \ where, i = 1, ...,p
\end{equation}
Here, $r_{i}$ is a point position and velocity measurement with respect to the radar sensor coordinate and p is the number of radar measurements at time step $k$.
On the other hand, LIDAR locate objects as 3D point clouds which helps to draw the object appearance completely or partially. For LIDAR data representation, six LIDARs are considered as one homogeneous sensor. The feature extraction task of LIDARs measurements are done by their build-in segmentation and feature extraction functionalists\cite{Cho_2014}. LIDAR measurement in $k^{th}$ second  is:
\begin{equation}
z^{L}(k) = \{l_{1},l_{2},...,l_{p}\} \ here, l_{i} = [\mathbf {x \ y \ \Phi \ \dot{x} \ \dot{y} \ w \ l}]^{T} \ where, i = 1, ...,q
\end{equation}
Here, $l_{i}$ contains the center of the box, the rotation ($\Phi$), velocity, width($w$), and length($l$) of the box. After LIDAR, cameras are the final visual sensor which could provide high definition images. Camera images are used to detect and track moving objects even static objects. In \cite{Cho_2014}, a vision based object detection technology developed to identify pedestrians, bicyclists and vehicles\cite{Cho_2014}. For sensor fusion purpose, the detected objects are represented using bounding boxes and treat them as measurements from vision sensors \cite{Cho_2014}. For camera image, if the camera image c captured in time step k,
\begin{equation}
z^{C}(k) = \{c_{1},c_{2},...,c_{p}\} \ here, c_{i} = [\mathbf {x_{1} \ y_{1} \ x_{2} \ y_{2} \ class}]^{T} \ where, i = 1, ...,r
\end{equation}
Here, $(x_{1} , y_{1})$ and $(x_{2} , y_{2})$ are the coordinates of the left-top and right-bottom point of a bounding box in image space, respectively and the class describes the object class such as pedestrian, bicyclist, or vehicle. r is the number of bounding box measurements at time step k.
Overall, each sensor have different way of representing data and the task performing characteristics is different from each other, but the fusion layer deal with them in the same in order to simplify the fusion process. The fusion layer captures data from three different types of sensors at time step k as:
\begin{equation}
z(k) = \{z^{R}(k), z^{L}(k), z^{C}(k)\}
\end{equation}
In the real world scenario, the measurements are captured asynchronously and published into the common communication channel at every time-stamp.

\subsubsection{Sensor Fusion}
In \cite{Cho_2014}, Extended Kalman Filter (EKF) is implemented in order to improve the tracking system. For optimization, the observation of sensor data task is done in the sequential-sensor manner where each sensor is observe independently and sequentially to perform EKF’s estimation task.
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{src/pic/obstacle_a.png}
        \caption{}
        \label{fig:obstacle_a}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{src/pic/obstacle_b.png}
        \caption{}
        \label{fig:obstacle_b}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{src/pic/obstacle_c.png}
        \caption{}
        \label{fig:obstacle_c}
    \end{subfigure}
    \caption{\ref{fig:obstacle_a}:, \ref{fig:obstacle_b}:, \ref{fig:obstacle_c}:}
    \label{fig:obstacle}
\end{figure}
\begin{enumerate}[label=\Alph*]
    \item Tracking Models: To demonstrate tracking models, two motion models are chosen as standard i.e., a point model ($M_{P}$), a 3D box model ($M_{B}$). To observe three kinds of sensors three observation models are introduced i.e., Radar($O_{R}$), LIDAR ($O_{L}$) and camera ($O_{C}$) observation model.
    \begin{description}
        \item[Motion Models:] For every moving objects, pedestrians, bicyclists, and cars in this case, the kinematics situations are not same. A pedestrian has the probability to move in any direction in any time. But for the bicyclists or cars, the movement is confined by non-holonomic constraints\cite{Cho_2014}. Motion estimation is performed using point model and 3D box model where it is assumed that a point model moves with a constant acceleration\cite{Cho_2014}. The 3D box model is a bicycle model wit its estimated 3D cuboid\cite{Cho_2014}.
        \item[Observation Models:] In order to capture the point target, the Radar observation model ($O_{R}$) is used. The LIDAR observation model ($O_{L}$) is responsible for modeling a box target. Finally the camera observation model ($O_{C}$) handle the bounding box measurements in the image plane. In practice, the bounding box are not used to update the motion estimation because the depth is uncertain. In \cite{Cho_2014}, the detection results are used to estimate the width and the height of an object and determines objects’ classes. But it can not be used to object initialization or terminate of objects. When the image frames are ready, the length and width is calculated by subtracting the pixel and the 3D height calculated based on camera geometry.
    \end{description}
    \item Data Association: The association of the current state with the previous state is a very critical task. To do the task optimally, some improvements has to be done in the  previous data association algorithm\cite{Cho_2014}.
    
    For camera observations(also called vision targets), the center of determined moving object is projected as either in a point or a box model on the next image frame under the pinhole camera model. After projection, a search is done due to find the nearest point that reduce the distance between the projected point and the middle of the bottom line of the detected bounding boxes. After that the camera observation and its object classification is instantiated. Figure \ref{fig:obstacle_a} shows the details about vision targets.
    
    For LIDAR observations(also called edge targets), a set of possible alignments of edge targets are generated depending on the predicted moving objects. There are four alignment for a box model and one for each point model\cite{Cho_2014}. The extracted targets are associated to the closest predicted one and which reduces the distance between the projected point and the middle of the bottom line of the detected bounding boxes. For utilization, edge targets tracking takes help from vision targets. For example, if the vision target focus on the rear view then it ignores the vehicles side view alignments. Figure \ref{fig:obstacle_b} shows the details about edge targets.
    
    For radar observations(also called point targets), a set of possible point targets is generated from the predicted moving object hypotheses\cite{Cho_2014}. Since radars are usually poor in determining a lateral position of an object, when a tracked object is modeled as a 3D box model, we generate multiple points along the contour of the box model. If an object is tracked through a point model, we generate a single point\cite{Cho_2014}. Figure \ref{fig:obstacle_c} shows the details about point targets.
    
    \item Movement Classification: It is the only goal of tracking is to know if the target is a moving or non-moving object. There are several scenarios of moving object. One object can move continuously, it may move a little bit and stop or stop a little bit then start. To track all of the scenarios, a series of states of a moving objects needs to be keep. In \cite{Cho_2014}, two movement flags are introduced. One is the movement history i.e., observed moving and not observed moving and another on is the movement state, i.e., moving and not moving\cite{Cho_2014}. As radar has build in functionality to track the moving objects, radar is the key component of tracking. Also the motion is compared with a threshold which is statistically determined by testing various vehicles. The movement history classification, the distance traveled is computed from the last time stamp that the object has been classified as not observed moving\cite{Cho_2014}. Here, the distance is also compared with a threshold. Though it is hard to find out the perfect threshold value, it is possible to get an realistic optimal value.
\end{enumerate}
\subsubsection{Experimental Outcome}
The improved version of the tracking system shows a very good performance in real time\cite{Cho_2014}. Some improved cases are described in the following. When a vehicle is tracked in the range of 150m, the tracking system track it as a point model. But when the vehicle comes near at least 40m from the host vehicle the tracking system automatically tracked it as a 3D box model. It is one of the most important feature of self driving car. Because the car should know the exact dimension of the moving object as it come near or go far\cite{Cho_2014}. On the other hand, for pedestrians/bicyclists the range is up to 20m on the experiment vehicle's path.
\section{Sensor-Independent Fusion Approach}
In the area of "Autonomous Driving", it is a very basic idea to use multiple sensors and to fuse the sensors data to determine something which can not be determined by just evaluating the sensors data separately. The sensors are hardware module which are accessed by a very complex software module to handle the sensor data and operate the vehicle. Meanwhile, different sensors module may have different software dependency which makes the changeability of sensor harder. There could be several reasons to change the sensors, but the main two reasons are: 
\begin{enumerate}
    \item Add more sensors to increase reliability or systems Field of View (FOV)\cite{Kunz_2015}
    \item Replacing costly sensors with the cheaper ones, with a reduced for example, to make the system available for cheaper cars. So, if there is a need of replacement or enhancement, the software module is also needs to be updated or redeveloped.
\end{enumerate}
So, if there is a need of replacement or enhancement, the software module is also needs to be updated or integrated with new code. In the paper \cite{Kunz_2015}, a modular multi-sensor sensor independent fusion system is proposed where sensor independent means it can accept any kind of sensor. It would be possible in that system that, changing the sensors does not involved software changes. This sensor independent approach ensures redundancy and replaceable using sensor-independent probabilistic interfaces\cite{Kunz_2015}. From the view point of performance, this system could provide good performance in the occurrence of temporary malfunction or degradation of individual sensors.

\subsection{Environment Setup}
Foe the experiment, a Mercedes-Benz E-Class car is selected and modified to enable autonomous features. The modification includes, A Paravan Space-Drive steering system which enables moving the steering by wire, a modified ESP(electronic stability program) control unit which enables to directly specify the break and acceleration torque\cite{Kunz_2015}. To perform the high level computational tasks, two computers are installed in the trunk and they could perform parallel.

\subsection{Sensors Used}
Three sensors are installed in the experiment vehicle. They are installed in a way that, it is hardly visible from outside and integrated in the body. There are three IBEO LUX laser scanners are set in the front bumper. They have the ability to scan maximum range of up to 200m at front and the angle of 210$^{\circ}$. There is also a front facing monochrome camera is installed behind the windshield with a resolution of 1392 x 1040 px. Moreover, the car is equipped with seral radar sensors. A Frequency Modulated Continuous Wave (FMCW) based Continental ARS 310 automotive long-range radar is mounted underneath the grille\cite{Kunz_2015}. The radars have the range of o.25m to 200m. The sensor coverage is visualized in the fig \ref{fig:sensor_covarage_Independent}. To cover the rear view, a radar is mounted centered behind the rear bumper and two reward facing cameras are mounted hehind the rear window. Besides all of these sensors, A real-time kinematic (RTK) system in combination with a differential GPS is used for mapping and evaluation\cite{Kunz_2015}.
\begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{src/pic/sensor_covarage_Independent.png}
  \caption{The front face of the vehicle is in the right direction.  Sensor coverage: cameras(blue), laser scanners (red), radar sensors (green) \cite{Kunz_2015}}
  \label{fig:sensor_covarage_Independent}
\end{figure}
\subsection{Sensor Fusion}
According to \cite{Kunz_2015}, in a multi-sensor system it is highly recommended to utilize all of the sensors data. One sensor may recover the draw backs of other sensors. For example, A radar has build in functionality to give accurately the position and velocity of moving object. But the yaw angle is uncertain. To recover this drawback of radar we could use a mono camera which can not provide velocity information but it can provide an accurate yaw angle. Though the direct coupling of sensors enables the system to get complete information, it makes the system sensor dependent\cite{Kunz_2015}.

\begin{figure}
  \centering
  \includegraphics[width=.6\textwidth]{src/pic/kunz_PS.png}
  \caption{Perception System Overview \cite{Kunz_2015}}
  \label{fig:kunz_PS}
\end{figure}
The proposed perception system in \cite{Kunz_2015} is a hierarchical modular environment perception system (HMEP). There are three layers of HMEP, they are: grid mapping, localization and object tracking. The fusion is applied in each layer individually with the sensor data and as a result each layer produces an element of the environment model. It is also allowed that one perception layer use the produced data from another layer in addition to sensor data. But this design is not deadlock free. To make it deadlock free the layers are designed hierarchically and it is only allowed that the results of lower layers may be used by the upper layers. The order is determined by the abstraction level of the environment model element. For example, tracked objects are the final abstract environment model and placed in the highest order. Meanwhile, the results from the perception layers could be redundant or even contradictory\cite{Kunz_2015}. To resolve this problem, a combination module is introduced which combines all elements to a consistent environmental model\cite{Kunz_2015}. The combination module expects that the result produced in the perception layers are probabilistic. For example, the results should contains the information of uncertainty\cite{Kunz_2015}. Moreover, the independent senor fusion needs some generic sensor interfaces which must be provided by the individual layers. The generic sensor interfaces enable the system to replace or enhance with sensors of same or even different types without changing the core of sensor fusion which is the main goal of this perception system.

\begin{enumerate}[label=\Alph*]
    \item Grid Mapping Layer: This layer turns the environment model into single grid cells and estimates each cells that how much the cell is occupied with the environment object\cite{Kunz_2015}. Classical occupancy grid mapping approach is followed for grid mapping which are not compatible for dynamic environments\cite{Kunz_2015}. Therefore, an improved  grid map is implemented which is capable of quickly adapting current situation. In the real implementation, the data of the three laser scanners is used as input. Actually, the grid map is used by the combination module to estimate static boundaries. New sensors can be integrated into the grid mapping layer by creating an inverse sensor model for the sensor as pre-processing filter as shown in Figure [grid-mapping]\cite{Kunz_2015}. The inverse sensor model determines the probability of occupancy of each grid cell by evaluating the sensor output which is called a measurement grid. In a nutshell, the grid mapping produce the result of estimated occupancy probability of every cell in the grid\cite{Kunz_2015}.
    \item Localization Layer: The location of the vehicle is estimated by the help of a concept called Monte-Carlo Localization (MCL) on a feature-based map representation. The feature map is extracted from the lower level grid map layer where the grid map is generated from three front facing leaser scanner using Maximally Stable Extremal Regions (MSERs). MSERs are robust features that are additionally extracted from gray-scale images within a pre-defined region of interest (ROI)\cite{Kunz_2015}. Grid map extracts features which may contain tree trunks, road signs, posts, etc. Grid map also can provide well-suited point landmarks for the localization process. On the other hand, camera extracts the lane markings\cite{Kunz_2015}. Due to generic approach, sensors can be replaced easily and even missing of one of them (leaser scanner or camera) could be handled. But  A missing laser scanner decreases the longitudinal precision, while a loss of camera measurements results in a larger lateral uncertainty\cite{Kunz_2015}.
    \item Multi-Object Tracking: The tracking of dynamic objects in environment is realized by using a generic multi-sensor information fusion module. Radar, laser scanner and  video cameras are used as sensors in the fusion module. Raw data collected from sensors may contain large number of extra information which makes the fusion process computationally costly. To optimize this, some pre-processing filtering algorithms are applied in the sensors raw data to obtain the object hypothesis. The tracking system, introduced in \cite{Kunz_2015}, is using Labeled MultiBernoulli (LMB) filter for pre-processing. Each sensor is connected to the LMB filter using sensor specific measurement model. The generic measurement model provides measurement functions, field of views, detection probabilities and false positive probabilities. This measurement model facilitate the sensor integration process in a way that new sensors can be easily integrated by just implementing the sensor specific measurement model which needs no change in LMB filter.
    \item Environment Model: The grid map, the localization result, the digital map and the object list are combined into a consistent environment model by the combination module. In a nutshell, the grid map is replaced by boundary lines which specify the driving space of the vehicle. Though the dynamic objects included in the grid map, they are just ignored while creating the boundary line.
\end{enumerate}

\subsection{Experimental Outcome}
The experimental route used in \cite{Kunz_2015} is only 5 km long which includes a variety of challenges for autonomous vehicles: traffic lights, crosswalks, and roundabouts. Additionally, half of the track does not contain any lane markings. Additionally huge amount of pedestrians and other road users make the driving experience more difficult. The speed limit was restricted from 50 to 70 km/h. The combination of developed grid map and object tracking smoothly detects all road users and other visible obstacles. The localization module provides a very high accuracy of vehicle position. In the experiment, it is also realized that laser scanner can provide more accurate position. If only video is available the localization error increases due to utilization of the flat world assumption. Besides, laser only system has also some weakness. But still it is possible to recover single sensor limitations. In critical weather conditions, the high resolution video camera is influenced in its performance. If the video camera miss detect the land marks, the localization module automatically relies on laser scanners.